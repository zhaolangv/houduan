"""
æ‰¹é‡é¢˜ç›®å¤„ç†æœåŠ¡ - ä½¿ç”¨æœ¬åœ°OCR + DeepSeekï¼Œæ”¯æŒé«˜å¹¶å‘
ä¸“é—¨ç”¨äºå¿«é€Ÿæ‰¹é‡å¤„ç†50+é“é¢˜
"""
import os
import json
import re
import time
import logging
from typing import Dict, List
from concurrent.futures import ThreadPoolExecutor, as_completed
from openai import OpenAI
from io import BytesIO
from difflib import SequenceMatcher

logger = logging.getLogger(__name__)

# DeepSeek é…ç½®
DEEPSEEK_API_KEY = os.getenv('DEEPSEEK_API_KEY', 'sk-7de12481a17045819fcf3a2838d884a1')
DEEPSEEK_API_BASE = 'https://api.deepseek.com/v1'
MODEL = 'deepseek-chat'

# ä»·æ ¼é…ç½®ï¼ˆå…ƒ/åƒtokenï¼‰
DEEPSEEK_PRICING = {'input': 0.00014, 'output': 0.00056}


def preprocess_ocr_text(raw_text: str) -> str:
    """å¿«é€Ÿé¢„å¤„ç†OCRæ–‡æœ¬ï¼Œè¿‡æ»¤æ˜æ˜¾çš„ç•Œé¢å…ƒç´ """
    if not raw_text:
        return raw_text
    
    lines = raw_text.split('\n')
    filtered_lines = []
    
    # ä¸¥æ ¼çš„ç•Œé¢å…ƒç´ å…³é”®è¯
    strict_interface_keywords = [
        'KB/s', 'é¦–é¡µ', 'æœ‹å‹', 'æ¶ˆæ¯', 'æˆ‘', 'æ‹åŒ', 'ç‚¹å‡»æ¨è',
        'ç²‰ç¬”æ­£ç¡®ç‡', 'åå›¾æ­£ç¡®ç‡', 'ç­”æ¡ˆä¸€æ ·', 'è§£æåœ¨ä½œå“', 'è§£æåœ¨ä½œå“ç®€',
        'å±•å¼€', 'æ”¶èµ·', 'åˆ†äº«', 'ç‚¹èµ', 'æ”¶è—', 'è¯„è®º',
        '@å…¬è€ƒè¡Œæµ‹æ¯æ—¥ä¸€ç»ƒ', 'å…¬è€ƒè¡Œæµ‹æ¯æ—¥ä¸€ç»ƒçš„æ©±çª—',
        'æ©±çª—|', 'ç‚¹å‡»æ¨è', 'ç¥å„ä½å›½è€ƒ', 'è¡Œæµ‹80', 'ç”³è®º85',
        'Never give up'
    ]
    
    option_markers = ['A.', 'B.', 'C.', 'D.', 'E.', 'F.', 'A ', 'B ', 'C ', 'D ']
    question_keywords = ['è¿™æ®µæ–‡å­—', 'æ„åœ¨è¯´æ˜', 'æ ¹æ®', 'ä»¥ä¸‹', 'é¢˜ç›®', 'é¢˜å¹²']
    
    for line in lines:
        line_stripped = line.strip()
        if not line_stripped:
            continue
        
        # è·³è¿‡æ˜ç¡®çš„ç•Œé¢å…ƒç´ 
        is_interface = any(keyword in line_stripped for keyword in strict_interface_keywords)
        if is_interface:
            continue
        
        # ä¿ç•™é€‰é¡¹æ ‡è®°
        has_option_marker = any(line_stripped.startswith(marker) for marker in option_markers)
        if has_option_marker:
            filtered_lines.append(line_stripped)
            continue
        
        # ä¿ç•™é¢˜å¹²å…³é”®è¯
        has_question_keyword = any(keyword in line_stripped for keyword in question_keywords)
        if has_question_keyword:
            filtered_lines.append(line_stripped)
            continue
        
        # ä¿ç•™è¾ƒé•¿çš„æ–‡æœ¬è¡Œ
        if len(line_stripped) > 3:
            if len(line_stripped) > 10 or any(p in line_stripped for p in ['ã€‚', 'ï¼Œ', 'ã€', 'ï¼›', 'ï¼Ÿ', 'ï¼š']):
                filtered_lines.append(line_stripped)
    
    return '\n'.join(filtered_lines)


def get_ocr_text(image_path_or_file, use_preprocess=True) -> Dict:
    """
    ç»Ÿä¸€ä½¿ç”¨æœ¬åœ°OCRè·å–æ–‡å­—
    
    Args:
        image_path_or_file: å›¾ç‰‡æ–‡ä»¶å¯¹è±¡æˆ–è·¯å¾„
        use_preprocess: æ˜¯å¦ä½¿ç”¨å›¾ç‰‡é¢„å¤„ç†ï¼ˆé»˜è®¤Trueï¼Œæ‰¹é‡å¤„ç†æ—¶å¯è®¾ä¸ºFalseæé«˜é€Ÿåº¦ï¼‰
    """
    import logging
    logger = logging.getLogger(__name__)
    
    from ocr_service import get_ocr_service
    ocr_service = get_ocr_service()
    
    if not ocr_service.ocr_engine:
        logger.warning("[OCR] âš ï¸ OCRå¼•æ“ä¸å¯ç”¨")
        return {'success': False, 'error': 'OCRä¸å¯ç”¨'}
    
    start = time.time()
    
    # è·å–æ–‡ä»¶åç”¨äºæ—¥å¿—
    if hasattr(image_path_or_file, 'name'):
        file_name = image_path_or_file.name
    elif isinstance(image_path_or_file, str):
        file_name = os.path.basename(image_path_or_file)
    else:
        file_name = 'æœªçŸ¥'
    
    logger.debug(f"[OCR] ğŸ” å¼€å§‹OCRè¯†åˆ«: {file_name}, é¢„å¤„ç†={'æ˜¯' if use_preprocess else 'å¦'}")
    
    # å¤„ç†æ–‡ä»¶å¯¹è±¡æˆ–è·¯å¾„
    if hasattr(image_path_or_file, 'read'):
        # æ˜¯æ–‡ä»¶å¯¹è±¡ï¼Œéœ€è¦å…ˆä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
        import tempfile
        temp_path = None
        try:
            image_path_or_file.seek(0)
            with tempfile.NamedTemporaryFile(delete=False, suffix='.jpg') as tmp_file:
                tmp_file.write(image_path_or_file.read())
                temp_path = tmp_file.name
            
            raw_text = ocr_service.extract_text(temp_path, use_preprocess=use_preprocess)
            elapsed = time.time() - start
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if temp_path and os.path.exists(temp_path):
                try:
                    os.unlink(temp_path)
                except:
                    pass
    else:
        # æ˜¯æ–‡ä»¶è·¯å¾„
        raw_text = ocr_service.extract_text(image_path_or_file, use_preprocess=use_preprocess)
        elapsed = time.time() - start
    
    elapsed_total = time.time() - start
    
    if raw_text:
        char_count = len(raw_text)
        logger.info(f"[OCR] âœ… OCRè¯†åˆ«æˆåŠŸ: {file_name}, æå–åˆ° {char_count} å­—ç¬¦, è€—æ—¶={elapsed_total:.2f}ç§’")
        logger.debug(f"[OCR] ğŸ“ OCRæ–‡æœ¬é¢„è§ˆï¼ˆå‰100å­—ç¬¦ï¼‰: {raw_text[:100]}...")
        return {
            'success': True,
            'raw_text': raw_text,
            'time': elapsed_total,
            'char_count': char_count
        }
    else:
        logger.warning(f"[OCR] âš ï¸ OCRæœªè¯†åˆ«åˆ°æ–‡å­—: {file_name}, è€—æ—¶={elapsed_total:.2f}ç§’")
        return {'success': False, 'error': 'OCRæœªè¯†åˆ«åˆ°æ–‡å­—', 'time': elapsed_total}


def call_deepseek_extract(ocr_text: str, include_classification: bool = True) -> Dict:
    """
    è°ƒç”¨DeepSeekæå–é¢˜ç›®å’Œé€‰é¡¹ï¼ŒåŒæ—¶è¿›è¡Œåˆ†ç±»å’Œåˆæ­¥ç­”æ¡ˆæå–
    
    Args:
        ocr_text: OCRè¯†åˆ«çš„æ–‡æœ¬
        include_classification: æ˜¯å¦åŒ…å«åˆ†ç±»å’Œåˆæ­¥ç­”æ¡ˆï¼ˆé»˜è®¤Trueï¼‰
    
    Returns:
        Dict: åŒ…å«é¢˜ç›®ã€é€‰é¡¹ã€åˆ†ç±»ã€åˆæ­¥ç­”æ¡ˆç­‰ä¿¡æ¯
    """
    import logging
    logger = logging.getLogger(__name__)
    
    client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url=DEEPSEEK_API_BASE)
    
    # é¢„å¤„ç†OCRæ–‡æœ¬
    preprocessed_text = preprocess_ocr_text(ocr_text)[:3000]  # é™åˆ¶é•¿åº¦
    
    logger.info(f"[AI] ğŸ¤– å‡†å¤‡è°ƒç”¨DeepSeek APIæå–é¢˜ç›®")
    logger.debug(f"[AI] ğŸ“ OCRæ–‡æœ¬é•¿åº¦: {len(ocr_text)}å­—ç¬¦, é¢„å¤„ç†å: {len(preprocessed_text)}å­—ç¬¦")
    
    if include_classification:
        # æç¤ºè¯ï¼ˆåŒ…å«åˆ†ç±»å’Œåˆæ­¥ç­”æ¡ˆï¼‰
        prompt = f"""ä»ä»¥ä¸‹OCRè¯†åˆ«æ–‡å­—ä¸­æå–é¢˜ç›®å’Œé€‰é¡¹ï¼Œå¹¶è¿›è¡Œåˆ†ç±»å’Œåˆæ­¥ç­”æ¡ˆåˆ†æï¼Œå¿½ç•¥æ‰€æœ‰ç•Œé¢å…ƒç´ ã€‚

OCRæ–‡å­—ï¼š
{preprocessed_text}

è¦æ±‚ï¼š
1. æå–å®Œæ•´çš„é¢˜ç›®å†…å®¹å’Œé€‰é¡¹
2. é¢˜å¹²å¿…é¡»å®Œæ•´ï¼ŒåŒ…æ‹¬æ‰€æœ‰æ®µè½å†…å®¹
3. é€‰é¡¹å¿…é¡»ä»¥"A. "ã€"B. "ã€"C. "ã€"D. "å¼€å¤´
4. åˆ¤æ–­é¢˜ç›®ç±»å‹ï¼šè¡Œæµ‹(è¨€è¯­ç†è§£ã€æ•°é‡å…³ç³»ã€åˆ¤æ–­æ¨ç†ã€èµ„æ–™åˆ†æã€å¸¸è¯†åˆ¤æ–­) æˆ– ç”³è®º
5. ç»™å‡ºåˆæ­¥ç­”æ¡ˆï¼ˆA/B/C/Dï¼‰å’Œç®€è¦ç†ç”±
6. ä¸è¦åŒ…å«ç•Œé¢å…ƒç´ 

è¿”å›JSONæ ¼å¼ï¼ˆåªè¿”å›JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ï¼‰ï¼š
{{
    "question_text": "å®Œæ•´çš„é¢˜å¹²å†…å®¹",
    "options": ["A. é€‰é¡¹A", "B. é€‰é¡¹B", "C. é€‰é¡¹C", "D. é€‰é¡¹D"],
    "question_type": "è¡Œæµ‹-è¨€è¯­ç†è§£" æˆ– "è¡Œæµ‹-æ•°é‡å…³ç³»" æˆ– "ç”³è®º" ç­‰,
    "preliminary_answer": "B",
    "answer_reason": "ç®€è¦çš„ç†ç”±è¯´æ˜"
}}"""
    else:
        # æç¤ºè¯ï¼ˆä»…æå–é¢˜ç›®å’Œé€‰é¡¹ï¼Œä¸åˆ†ç±»ï¼‰
        prompt = f"""ä»ä»¥ä¸‹OCRè¯†åˆ«æ–‡å­—ä¸­æå–é¢˜ç›®å’Œé€‰é¡¹ï¼Œå¿½ç•¥æ‰€æœ‰ç•Œé¢å…ƒç´ ã€‚

OCRæ–‡å­—ï¼š
{preprocessed_text}

è¦æ±‚ï¼š
1. åªæå–é¢˜ç›®å†…å®¹å’Œé€‰é¡¹
2. é¢˜å¹²å¿…é¡»å®Œæ•´ï¼ŒåŒ…æ‹¬æ‰€æœ‰æ®µè½å†…å®¹
3. é€‰é¡¹å¿…é¡»ä»¥"A. "ã€"B. "ã€"C. "ã€"D. "å¼€å¤´
4. ä¸è¦åŒ…å«ç•Œé¢å…ƒç´ 

è¿”å›JSONæ ¼å¼ï¼ˆåªè¿”å›JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ï¼‰ï¼š
{{
    "question_text": "å®Œæ•´çš„é¢˜å¹²å†…å®¹",
    "options": ["A. é€‰é¡¹A", "B. é€‰é¡¹B", "C. é€‰é¡¹C", "D. é€‰é¡¹D"]
}}"""
    
    start_time = time.time()
    api_request_start = time.time()
    
    # è®°å½•APIè¯·æ±‚ä¿¡æ¯
    logger.info(f"[AI] ğŸš€ å¼€å§‹è°ƒç”¨DeepSeek API (æ¨¡å‹: {MODEL})")
    logger.info(f"[AI] ğŸ“‹ APIä¿¡æ¯: provider=DeepSeek, model={MODEL}, base_url={DEEPSEEK_API_BASE}")
    logger.info(f"[AI] ğŸ“ è¯·æ±‚å‚æ•°: prompté•¿åº¦={len(prompt)}å­—ç¬¦, include_classification={include_classification}, max_tokens={2000 if include_classification else 1500}, temperature=0.1")
    
    try:
        response = client.chat.completions.create(
            model=MODEL,
            messages=[
                {
                    "role": "system", 
                    "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é¢˜ç›®æå–å’Œåˆ†æåŠ©æ‰‹ï¼Œæ“…é•¿ä»OCRæ–‡å­—ä¸­å‡†ç¡®æå–å®Œæ•´çš„é¢˜ç›®å’Œé€‰é¡¹ï¼Œå¹¶è¿›è¡Œé¢˜ç›®åˆ†ç±»å’Œåˆæ­¥ç­”æ¡ˆåˆ†æã€‚åªè¿”å›JSONæ ¼å¼ã€‚"
                },
                {"role": "user", "content": prompt}
            ],
            temperature=0.1,
            max_tokens=2000 if include_classification else 1500,
            timeout=30
        )
        
        api_request_time = time.time() - api_request_start
        elapsed = time.time() - start_time
        content = response.choices[0].message.content.strip()
        
        # ç»Ÿè®¡tokenå’Œè´¹ç”¨
        input_tokens = response.usage.prompt_tokens if hasattr(response, 'usage') else 0
        output_tokens = response.usage.completion_tokens if hasattr(response, 'usage') else 0
        total_tokens = input_tokens + output_tokens
        cost = (input_tokens / 1000 * DEEPSEEK_PRICING['input']) + (output_tokens / 1000 * DEEPSEEK_PRICING['output'])
        
        # è®°å½•APIå“åº”ä¿¡æ¯
        response_length = len(content) if content else 0
        logger.info(f"[AI] âœ… DeepSeek APIè°ƒç”¨æˆåŠŸ")
        logger.info(f"[AI] â±ï¸  è€—æ—¶ç»Ÿè®¡: APIè¯·æ±‚={api_request_time:.2f}ç§’, æ€»è®¡={elapsed:.2f}ç§’")
        logger.info(f"[AI] ğŸ“Š å“åº”ç»Ÿè®¡: å†…å®¹é•¿åº¦={response_length}å­—ç¬¦, prompt_tokens={input_tokens}, completion_tokens={output_tokens}, total_tokens={total_tokens}")
        logger.info(f"[AI] ğŸ’° è´¹ç”¨: Â¥{cost:.6f}")
        if response_length > 0:
            logger.debug(f"[AI] ğŸ“ å“åº”å†…å®¹é¢„è§ˆï¼ˆå‰300å­—ç¬¦ï¼‰:\n{content[:300]}...")
        
        # è§£æJSON
        json_match = re.search(r'\{[\s\S]*\}', content)
        if json_match:
            try:
                parsed_result = json.loads(json_match.group())
                question_text = parsed_result.get('question_text', '').strip()
                options = parsed_result.get('options', [])
                
                # æ ¼å¼åŒ–é€‰é¡¹
                formatted_options = []
                for i, opt in enumerate(options):
                    opt_str = str(opt).strip()
                    if not re.match(r'^[A-F]\.?\s', opt_str):
                        opt_str = f"{chr(65+i)}. {opt_str}"
                    formatted_options.append(opt_str)
                
                result = {
                    'success': True,
                    'question_text': question_text,
                    'options': formatted_options,
                    'time': elapsed,
                    'input_tokens': input_tokens,
                    'output_tokens': output_tokens,
                    'total_tokens': total_tokens,
                    'cost': cost
                }
                
                # å¦‚æœæœ‰åˆ†ç±»å’Œåˆæ­¥ç­”æ¡ˆä¿¡æ¯ï¼Œæ·»åŠ åˆ°ç»“æœä¸­
                if include_classification:
                    result['question_type'] = parsed_result.get('question_type', 'TEXT')
                    result['preliminary_answer'] = parsed_result.get('preliminary_answer', '')
                    result['answer_reason'] = parsed_result.get('answer_reason', '')
                
                logger.info(f"[AI] âœ… é¢˜ç›®æå–æˆåŠŸ: é¢˜å¹²é•¿åº¦={len(question_text)}å­—ç¬¦, é€‰é¡¹æ•°={len(formatted_options)}, ç±»å‹={result.get('question_type', 'N/A')}")
                
                return result
            except json.JSONDecodeError as e:
                return {
                    'success': False,
                    'error': f'JSONè§£æå¤±è´¥: {str(e)}',
                    'time': elapsed,
                    'total_tokens': total_tokens,
                    'cost': cost,
                    'raw_response': content[:500]
                }
        else:
            return {
                'success': False,
                'error': 'æœªæ‰¾åˆ°JSONæ ¼å¼å“åº”',
                'time': elapsed,
                'total_tokens': total_tokens,
                'cost': cost,
                'raw_response': content[:500]
            }
    
    except Exception as e:
        elapsed = time.time() - start_time
        api_elapsed = time.time() - api_request_start if 'api_request_start' in locals() else 0
        error_type = type(e).__name__
        error_msg = str(e)
        logger.error(f"[AI] âŒ DeepSeek APIè°ƒç”¨å¤±è´¥: {error_type}: {error_msg}, APIè€—æ—¶={api_elapsed:.2f}ç§’, æ€»è®¡={elapsed:.2f}ç§’", exc_info=True)
        return {
            'success': False,
            'error': f'APIè°ƒç”¨å¤±è´¥: {str(e)}',
            'time': elapsed
        }


def check_duplicate_from_ocr_text(ocr_text: str, app=None) -> Dict:
    """
    åŸºäºOCRæ–‡æœ¬æ£€æµ‹é‡å¤é¢˜ç›®ï¼Œå¦‚æœæ‰¾åˆ°åˆ™ç›´æ¥ä»é¢˜åº“æå–
    
    æ³¨æ„ï¼šæ­¤å‡½æ•°éœ€è¦åœ¨Flaskåº”ç”¨ä¸Šä¸‹æ–‡ä¸­è°ƒç”¨
    
    Args:
        ocr_text: OCRè¯†åˆ«çš„æ–‡æœ¬
        app: Flask åº”ç”¨å®ä¾‹ï¼ˆå¦‚æœä¸åœ¨åº”ç”¨ä¸Šä¸‹æ–‡ä¸­ï¼Œéœ€è¦æä¾›ï¼‰
    
    Returns:
        Dict: {
            'is_duplicate': bool,  # æ˜¯å¦æ‰¾åˆ°é‡å¤é¢˜
            'question': Questionå¯¹è±¡æˆ–None,  # é‡å¤çš„é¢˜ç›®å¯¹è±¡
            'similarity': float  # ç›¸ä¼¼åº¦åˆ†æ•°
        }
    """
    if not ocr_text or len(ocr_text.strip()) < 10:
        return {'is_duplicate': False, 'question': None, 'similarity': 0.0}
    
    try:
        from flask import has_app_context
        
        # å¦‚æœæ²¡æœ‰åº”ç”¨ä¸Šä¸‹æ–‡ä¸”æœ‰ app å®ä¾‹ï¼Œåˆ›å»ºåº”ç”¨ä¸Šä¸‹æ–‡
        if not has_app_context():
            if app is None:
                logger.warning("[BatchService] é‡å¤æ£€æµ‹å¤±è´¥: æ— åº”ç”¨ä¸Šä¸‹æ–‡ä¸”æœªæä¾› app å®ä¾‹")
                return {'is_duplicate': False, 'question': None, 'similarity': 0.0}
            
            with app.app_context():
                return _check_duplicate_in_context(ocr_text)
        else:
            # å·²åœ¨åº”ç”¨ä¸Šä¸‹æ–‡ä¸­
            return _check_duplicate_in_context(ocr_text)
            
    except Exception as e:
        logger.warning(f"[BatchService] é‡å¤æ£€æµ‹å¤±è´¥: {e}")
        return {'is_duplicate': False, 'question': None, 'similarity': 0.0}


def _check_duplicate_in_context(ocr_text: str) -> Dict:
    """åœ¨åº”ç”¨ä¸Šä¸‹æ–‡ä¸­æ‰§è¡Œé‡å¤æ£€æµ‹ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼šå¿«é€Ÿæ£€æŸ¥ï¼‰"""
    from question_service_v2 import QuestionService
    from models_v2 import Question
    
    # å¿«é€Ÿæ£€æŸ¥ï¼šå¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰é¢˜ç›®ï¼Œç›´æ¥è·³è¿‡
    try:
        question_count = Question.query.count()
        logger.info(f"[BatchService] ğŸ” é‡å¤æ£€æµ‹: æ•°æ®åº“ä¸­ç°æœ‰ {question_count} é“é¢˜ç›®")
        if question_count == 0:
            logger.info("[BatchService] âš ï¸ æ•°æ®åº“ä¸ºç©ºï¼Œè·³è¿‡é‡å¤æ£€æµ‹")
            return {'is_duplicate': False, 'question': None, 'similarity': 0.0}
    except Exception as e:
        logger.warning(f"[BatchService] æ£€æŸ¥æ•°æ®åº“é¢˜ç›®æ•°é‡å¤±è´¥: {e}ï¼Œç»§ç»­æ‰§è¡Œé‡å¤æ£€æµ‹")
    
    question_service = QuestionService()
    
    # è®°å½•OCRæ–‡æœ¬é•¿åº¦ç”¨äºæ—¥å¿—
    ocr_text_length = len(ocr_text) if ocr_text else 0
    logger.info(f"[BatchService] ğŸ” å¼€å§‹é‡å¤æ£€æµ‹: OCRæ–‡æœ¬é•¿åº¦={ocr_text_length}å­—ç¬¦, ç›¸ä¼¼åº¦é˜ˆå€¼=0.85")
    
    # åœ¨åº”ç”¨ä¸Šä¸‹æ–‡ä¸­è°ƒç”¨ï¼ˆå·²ä¼˜åŒ–ï¼šåªæŸ¥è¯¢æœ€è¿‘1000æ¡ï¼‰
    duplicate_check_start = time.time()
    duplicate_question, similarity = question_service.find_duplicate_by_text_similarity(
        ocr_text, 
        threshold=0.85
    )
    duplicate_check_time = time.time() - duplicate_check_start
    
    if duplicate_question and similarity >= 0.85:
        logger.info(f"[BatchService] âœ… æ£€æµ‹åˆ°é‡å¤é¢˜ç›®: ID={duplicate_question.id}, ç›¸ä¼¼åº¦={similarity:.3f}, æ£€æµ‹è€—æ—¶={duplicate_check_time:.2f}ç§’")
        return {
            'is_duplicate': True,
            'question': duplicate_question,
            'similarity': similarity
        }
    
    logger.info(f"[BatchService] â„¹ï¸ æœªå‘ç°é‡å¤é¢˜ç›® (ç›¸ä¼¼åº¦={similarity:.3f if similarity else 0.0:.3f}, é˜ˆå€¼=0.85, æ£€æµ‹è€—æ—¶={duplicate_check_time:.2f}ç§’)")
    return {'is_duplicate': False, 'question': None, 'similarity': similarity or 0.0}


def extract_from_duplicate_question(question, similarity: float) -> Dict:
    """
    ä»é‡å¤é¢˜ç›®ä¸­æå–ä¿¡æ¯ï¼ˆç›´æ¥ä»é¢˜åº“æå–ï¼Œæ— éœ€OCRå’ŒAIï¼‰
    
    Args:
        question: Questionå¯¹è±¡
        similarity: ç›¸ä¼¼åº¦åˆ†æ•°
    
    Returns:
        Dict: æå–ç»“æœï¼ˆæ ¼å¼ä¸process_single_questionä¸€è‡´ï¼‰
    """
    import json
    
    options = question.options
    if isinstance(options, str):
        try:
            options = json.loads(options)
        except:
            options = []
    elif not isinstance(options, list):
        options = []
    
    return {
        'success': True,
        'question_text': question.question_text or '',
        'options': options,
        'raw_text': question.raw_text or '',
        'question_type': question.question_type or 'TEXT',
        'question_id': str(question.id),
        'is_duplicate': True,
        'similarity': similarity,
        'ocr_time': 0,  # ä»é¢˜åº“æå–ï¼Œæ— éœ€OCR
        'ai_time': 0,   # ä»é¢˜åº“æå–ï¼Œæ— éœ€AI
        'total_time': 0.01,  # å‡ ä¹ç¬é—´å®Œæˆ
        'input_tokens': 0,
        'output_tokens': 0,
        'total_tokens': 0,
        'cost': 0.0,
        'extraction_method': 'database_cache'  # æ ‡è®°ä»æ•°æ®åº“æå–
    }


def process_single_question(image_file, question_index: int = None, frontend_ocr_text: str = None, app=None) -> Dict:
    """
    å¤„ç†å•é“é¢˜ï¼ˆä¸€æ¬¡å‘é€ä¸€é“é¢˜ï¼‰
    
    æ³¨æ„ï¼šæ­¤å‡½æ•°åœ¨å¹¶å‘çº¿ç¨‹ä¸­è°ƒç”¨æ—¶ï¼Œéœ€è¦ app å‚æ•°æ¥åˆ›å»ºåº”ç”¨ä¸Šä¸‹æ–‡
    
    Args:
        image_file: å›¾ç‰‡æ–‡ä»¶å¯¹è±¡æˆ–è·¯å¾„
        question_index: é¢˜ç›®ç´¢å¼•ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        frontend_ocr_text: å‰ç«¯æä¾›çš„OCRç»“æœï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›åˆ™å…ˆæ£€æµ‹é‡å¤ï¼‰
        app: Flask åº”ç”¨å®ä¾‹ï¼ˆç”¨äºåœ¨å¹¶å‘çº¿ç¨‹ä¸­åˆ›å»ºåº”ç”¨ä¸Šä¸‹æ–‡ï¼‰
    
    Returns:
        Dict: å¤„ç†ç»“æœ
    """
    import logging
    logger = logging.getLogger(__name__)
    
    index_str = f"é¢˜ç›®{question_index+1}" if question_index is not None else "é¢˜ç›®"
    question_start_time = time.time()
    
    logger.info(f"[BatchService] ğŸš€ {index_str}: å¼€å§‹å¤„ç†...")
    
    try:
        # 0. å¦‚æœå‰ç«¯æä¾›äº†OCRç»“æœï¼Œå…ˆæ£€æµ‹é‡å¤
        if frontend_ocr_text and len(frontend_ocr_text.strip()) >= 10:
            logger.info(f"[BatchService] {index_str}: ğŸ” å‰ç«¯æä¾›äº†OCRç»“æœï¼ˆ{len(frontend_ocr_text)}å­—ç¬¦ï¼‰ï¼Œå…ˆæ£€æµ‹é‡å¤...")
            duplicate_check = check_duplicate_from_ocr_text(frontend_ocr_text, app=app)
            
            if duplicate_check['is_duplicate']:
                logger.info(f"[BatchService] {index_str}: âœ… æ£€æµ‹åˆ°é‡å¤é¢˜ï¼Œç›´æ¥ä»é¢˜åº“æå– (ç›¸ä¼¼åº¦={duplicate_check.get('similarity', 0):.3f})")
                result = extract_from_duplicate_question(
                    duplicate_check['question'],
                    duplicate_check['similarity']
                )
                result['index'] = question_index
                return result
            else:
                logger.info(f"[BatchService] {index_str}: â„¹ï¸ æœªæ£€æµ‹åˆ°é‡å¤ï¼Œç»§ç»­å¤„ç†")
        
        # 1. OCRè¯†åˆ«ï¼ˆå¦‚æœå‰ç«¯æ²¡æœ‰æä¾›ï¼Œæˆ–æ£€æµ‹æœªå‘ç°é‡å¤ï¼‰
        ocr_start = time.time()
        
        # å¦‚æœå‰ç«¯æä¾›äº†OCRç»“æœï¼Œä½¿ç”¨å‰ç«¯çš„ï¼›å¦åˆ™ä½¿ç”¨æœ¬åœ°OCR
        if frontend_ocr_text and len(frontend_ocr_text.strip()) >= 10:
            ocr_result = {
                'success': True,
                'raw_text': frontend_ocr_text,
                'time': 0,  # å‰ç«¯OCRï¼Œæ—¶é—´ä¸è®¡å…¥
                'char_count': len(frontend_ocr_text)
            }
            ocr_time = 0
            logger.info(f"[BatchService] {index_str}: ä½¿ç”¨å‰ç«¯OCRç»“æœï¼ˆ{ocr_result['char_count']}å­—ç¬¦ï¼‰")
        else:
            # ä½¿ç”¨æœ¬åœ°OCRï¼ˆè·³è¿‡é¢„å¤„ç†ä»¥æé«˜é€Ÿåº¦ï¼Œæ‰¹é‡å¤„ç†æ—¶é€Ÿåº¦ä¼˜å…ˆï¼‰
            ocr_result = get_ocr_text(image_file, use_preprocess=False)
            ocr_time = time.time() - ocr_start
        
        if not ocr_result['success']:
            return {
                'success': False,
                'error': f"OCRå¤±è´¥: {ocr_result.get('error')}",
                'ocr_time': ocr_time,
                'ai_time': 0,
                'total_time': ocr_time
            }
        
        # å†æ¬¡æ£€æµ‹é‡å¤ï¼ˆä½¿ç”¨æœ¬åœ°OCRç»“æœï¼‰
        if not frontend_ocr_text:  # å¦‚æœä¹‹å‰æ²¡ç”¨å‰ç«¯OCRæ£€æµ‹è¿‡
            logger.info(f"[BatchService] {index_str}: ğŸ” ä½¿ç”¨æœ¬åœ°OCRç»“æœè¿›è¡Œé‡å¤æ£€æµ‹ (OCRæ–‡æœ¬é•¿åº¦={len(ocr_result.get('raw_text', ''))}å­—ç¬¦)...")
            
            # æ£€æŸ¥ app æ˜¯å¦å¯ç”¨
            if app is None:
                logger.warning(f"[BatchService] {index_str}: âš ï¸ app å‚æ•°ä¸º Noneï¼Œè·³è¿‡æ•°æ®åº“å»é‡æ£€æµ‹")
            else:
                duplicate_check = check_duplicate_from_ocr_text(ocr_result['raw_text'], app=app)
                if duplicate_check['is_duplicate']:
                    logger.info(f"[BatchService] {index_str}: âœ… æ£€æµ‹åˆ°é‡å¤é¢˜ï¼Œç›´æ¥ä»é¢˜åº“æå– (ç›¸ä¼¼åº¦={duplicate_check.get('similarity', 0):.3f})")
                    result = extract_from_duplicate_question(
                        duplicate_check['question'],
                        duplicate_check['similarity']
                    )
                    result['ocr_time'] = ocr_time  # ä¿ç•™OCRæ—¶é—´
                    result['index'] = question_index
                    return result
                else:
                    similarity = duplicate_check.get('similarity', 0.0)
                    logger.info(f"[BatchService] {index_str}: â„¹ï¸ æœªæ£€æµ‹åˆ°é‡å¤ (æœ€é«˜ç›¸ä¼¼åº¦={similarity:.3f}, é˜ˆå€¼=0.85)ï¼Œç»§ç»­AIæå–")
        
        # 2. AIæå–ï¼ˆå•é¢˜å•è¯·æ±‚ï¼ŒåŒ…å«åˆ†ç±»å’Œåˆæ­¥ç­”æ¡ˆï¼‰
        ai_result = call_deepseek_extract(ocr_result['raw_text'], include_classification=True)
        
        # åˆå¹¶ç»“æœ
        total_time = time.time() - question_start_time
        result = {
            'success': ai_result.get('success', False),
            'ocr_time': ocr_time,
            'ai_time': ai_result.get('time', 0),
            'total_time': total_time
        }
        
        if ai_result.get('success'):
            result.update({
                'question_text': ai_result.get('question_text', ''),
                'options': ai_result.get('options', []),
                'raw_text': ocr_result.get('raw_text', ''),
                'input_tokens': ai_result.get('input_tokens', 0),
                'output_tokens': ai_result.get('output_tokens', 0),
                'total_tokens': ai_result.get('total_tokens', 0),
                'cost': ai_result.get('cost', 0)
            })
            
            # æ·»åŠ åˆ†ç±»å’Œåˆæ­¥ç­”æ¡ˆä¿¡æ¯
            if 'question_type' in ai_result:
                result['question_type'] = ai_result.get('question_type', 'TEXT')
            if 'preliminary_answer' in ai_result:
                result['preliminary_answer'] = ai_result.get('preliminary_answer', '')
            if 'answer_reason' in ai_result:
                result['answer_reason'] = ai_result.get('answer_reason', '')
            
            logger.info(f"[BatchService] âœ… {index_str}: å¤„ç†æˆåŠŸ, æ€»è€—æ—¶={total_time:.2f}ç§’ (OCR={ocr_time:.2f}ç§’, AI={ai_result.get('time', 0):.2f}ç§’)")
        else:
            result['error'] = ai_result.get('error', 'æœªçŸ¥é”™è¯¯')
            logger.warning(f"[BatchService] âŒ {index_str}: å¤„ç†å¤±è´¥ - {result['error']}, æ€»è€—æ—¶={total_time:.2f}ç§’")
        
        return result
    
    except Exception as e:
        total_time = time.time() - question_start_time
        error_type = type(e).__name__
        logger.error(f"[BatchService] âŒ {index_str}: å¤„ç†å¼‚å¸¸ - {error_type}: {str(e)}, è€—æ—¶={total_time:.2f}ç§’", exc_info=True)
        return {
            'success': False,
            'error': f'å¤„ç†å¼‚å¸¸: {str(e)}',
            'ocr_time': 0,
            'ai_time': 0,
            'total_time': total_time
        }


def process_batch_concurrent(image_files: List, frontend_ocr_texts: List[str] = None, max_workers: int = 10, app=None, progress_callback=None) -> Dict:
    """
    å¹¶å‘æ‰¹é‡å¤„ç†å¤šé“é¢˜ï¼ˆæ¯é“é¢˜ç‹¬ç«‹è¯·æ±‚ï¼‰
    
    Args:
        image_files: å›¾ç‰‡æ–‡ä»¶å¯¹è±¡æˆ–è·¯å¾„åˆ—è¡¨
        frontend_ocr_texts: å‰ç«¯æä¾›çš„OCRç»“æœåˆ—è¡¨ï¼ˆå¯é€‰ï¼Œä¸image_filesä¸€ä¸€å¯¹åº”ï¼‰
        max_workers: å¹¶å‘æ•°ï¼ˆæ¨è10-20ï¼Œ50é¢˜çº¦2-3åˆ†é’Ÿï¼‰
        app: Flask åº”ç”¨å®ä¾‹ï¼ˆå¿…éœ€ï¼Œç”¨äºåœ¨å¹¶å‘çº¿ç¨‹ä¸­åˆ›å»ºåº”ç”¨ä¸Šä¸‹æ–‡ï¼‰
        progress_callback: è¿›åº¦æ›´æ–°å›è°ƒå‡½æ•° callback(completed, total, failed)
    
    Returns:
        Dict: {
            'results': List[Dict],  # æ¯é“é¢˜çš„å¤„ç†ç»“æœ
            'total': int,
            'success_count': int,
            'failed_count': int,
            'total_time': float,
            'avg_time_per_question': float,
            'total_cost': float
        }
    """
    if app is None:
        # å°è¯•ä» Flask è·å–å½“å‰åº”ç”¨
        try:
            from flask import current_app
            app = current_app._get_current_object()
            logger.info("[BatchService] ä» Flask current_app è·å–åˆ° app å¯¹è±¡")
        except:
            logger.warning("[BatchService] è­¦å‘Š: æœªæä¾› app å‚æ•°ï¼Œé‡å¤æ£€æµ‹åŠŸèƒ½å°†ä¸å¯ç”¨")
    else:
        logger.info("[BatchService] âœ… å·²æä¾› app å‚æ•°ï¼Œé‡å¤æ£€æµ‹åŠŸèƒ½å¯ç”¨")
    
    total_start = time.time()
    results = []
    total_cost = 0.0
    
    logger.info(f"[BatchService] âš™ï¸ æ‰¹é‡å¤„ç†å‚æ•°: {len(image_files)} å¼ å›¾ç‰‡, å¹¶å‘æ•°: {max_workers}, app: {app is not None}")
    
    # å¤„ç†å‰ç«¯OCRç»“æœåˆ—è¡¨
    if frontend_ocr_texts is None:
        frontend_ocr_texts = [None] * len(image_files)
    elif len(frontend_ocr_texts) < len(image_files):
        # å¦‚æœå‰ç«¯OCRç»“æœæ•°é‡ä¸è¶³ï¼Œç”¨Noneå¡«å……
        frontend_ocr_texts = frontend_ocr_texts + [None] * (len(image_files) - len(frontend_ocr_texts))
    else:
        # å¦‚æœå‰ç«¯OCRç»“æœæ•°é‡è¿‡å¤šï¼Œæˆªæ–­
        frontend_ocr_texts = frontend_ocr_texts[:len(image_files)]
    
    logger.info(f"[BatchService] å¼€å§‹æ‰¹é‡å¤„ç† {len(image_files)} é“é¢˜ï¼Œå¹¶å‘æ•°: {max_workers}")
    if any(ocr for ocr in frontend_ocr_texts if ocr):
        logger.info(f"[BatchService] å…¶ä¸­ {sum(1 for ocr in frontend_ocr_texts if ocr)} é“é¢˜æä¾›äº†å‰ç«¯OCRç»“æœ")
    
    # æ£€æŸ¥æ•°æ®åº“ä¸­çš„é¢˜ç›®æ•°é‡ï¼ˆç”¨äºå»é‡æ£€æµ‹ï¼‰
    try:
        if app:
            with app.app_context():
                from models_v2 import Question
                db_question_count = Question.query.count()
                logger.info(f"[BatchService] ğŸ“Š æ•°æ®åº“çŠ¶æ€: ç°æœ‰ {db_question_count} é“é¢˜ç›®ï¼Œå°†è¿›è¡Œå»é‡æ£€æµ‹")
        else:
            logger.warning(f"[BatchService] âš ï¸ æœªæä¾› app å‚æ•°ï¼Œæ— æ³•æ£€æŸ¥æ•°æ®åº“çŠ¶æ€ï¼Œå»é‡æ£€æµ‹å¯èƒ½ä¸å¯ç”¨")
    except Exception as e:
        logger.warning(f"[BatchService] âš ï¸ æ£€æŸ¥æ•°æ®åº“çŠ¶æ€å¤±è´¥: {e}ï¼Œå»é‡æ£€æµ‹å¯èƒ½å—å½±å“")
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡ï¼ˆä¼ é€’ app å‚æ•°ï¼‰
        future_to_idx = {
            executor.submit(process_single_question, img_file, idx, frontend_ocr_texts[idx], app=app): idx
            for idx, img_file in enumerate(image_files)
        }
        
        # å¤„ç†ç»“æœ
        completed = 0
        failed = 0
        processed_count = 0  # å·²å¤„ç†çš„æ€»æ•°ï¼ˆæˆåŠŸ+å¤±è´¥ï¼‰
        
        logger.info(f"[BatchService] ğŸ“‹ å¼€å§‹å¤„ç† {len(image_files)} é“é¢˜ç›®...")
        
        for future in as_completed(future_to_idx):
            idx = future_to_idx[future]
            try:
                result = future.result()
                result['index'] = idx
                results.append(result)
                processed_count += 1
                
                if result.get('success'):
                    total_cost += result.get('cost', 0)
                    completed += 1
                    logger.info(
                        f"[BatchService] âœ… é¢˜ç›®{idx+1}/{len(image_files)}: "
                        f"æˆåŠŸ (æ€»è€—æ—¶:{result.get('total_time', 0):.2f}ç§’, "
                        f"OCR:{result.get('ocr_time', 0):.2f}ç§’, "
                        f"AI:{result.get('ai_time', 0):.2f}ç§’, "
                        f"è´¹ç”¨:Â¥{result.get('cost', 0):.6f})"
                    )
                else:
                    failed += 1
                    logger.warning(
                        f"[BatchService] âŒ é¢˜ç›®{idx+1}/{len(image_files)}: "
                        f"å¤±è´¥ - {result.get('error', 'unknown')}"
                    )
                
                # æ›´æ–°è¿›åº¦ï¼ˆæ¯æ¬¡å®Œæˆä¸€é“é¢˜åç«‹å³æ›´æ–°ï¼‰
                if progress_callback:
                    try:
                        progress_callback(completed, len(image_files), failed)
                        logger.debug(f"[BatchService] ğŸ“Š å·²è°ƒç”¨è¿›åº¦å›è°ƒ: completed={completed}, total={len(image_files)}, failed={failed}")
                    except Exception as e:
                        logger.error(f"[BatchService] âŒ è¿›åº¦æ›´æ–°å›è°ƒå¤±è´¥: {e}", exc_info=True)
                else:
                    logger.debug(f"[BatchService] âš ï¸ è¿›åº¦å›è°ƒå‡½æ•°æœªæä¾›")
            
            except Exception as e:
                processed_count += 1
                failed += 1
                results.append({
                    'success': False,
                    'index': idx,
                    'error': f'å¤„ç†å¼‚å¸¸: {str(e)}',
                    'ocr_time': 0,
                    'ai_time': 0,
                    'total_time': 0
                })
                logger.error(f"[BatchService] âŒ é¢˜ç›®{idx+1}/{len(image_files)}: å¼‚å¸¸ - {str(e)}", exc_info=True)
                
                # æ›´æ–°è¿›åº¦
                if progress_callback:
                    try:
                        progress_callback(completed, len(image_files), failed)
                        logger.debug(f"[BatchService] ğŸ“Š å·²è°ƒç”¨è¿›åº¦å›è°ƒ(å¼‚å¸¸): completed={completed}, total={len(image_files)}, failed={failed}")
                    except Exception as e2:
                        logger.error(f"[BatchService] âŒ è¿›åº¦æ›´æ–°å›è°ƒå¤±è´¥(å¼‚å¸¸): {e2}", exc_info=True)
        
        logger.info(f"[BatchService] ğŸ“Š æ‰€æœ‰é¢˜ç›®å¤„ç†å®Œæˆ: æ€»è®¡={processed_count}, æˆåŠŸ={completed}, å¤±è´¥={failed}")
    
    # æŒ‰ç´¢å¼•æ’åºï¼Œä¿æŒåŸå§‹é¡ºåº
    results.sort(key=lambda x: x.get('index', 0))
    
    # ğŸ” åŒä¸€æ‰¹æ¬¡å†…çš„å»é‡æ£€æµ‹ï¼ˆå¤„ç†å®Œæˆåï¼Œæ£€æµ‹ç»“æœä¸­çš„é‡å¤é¢˜ç›®ï¼‰
    logger.info(f"[BatchService] ğŸ” å¼€å§‹æ£€æµ‹åŒä¸€æ‰¹æ¬¡å†…çš„é‡å¤é¢˜ç›®...")
    batch_duplicate_count = 0
    
    # ä½¿ç”¨ä¸ question_service_v2 ç›¸åŒçš„æ–‡æœ¬ç›¸ä¼¼åº¦ç®—æ³•
    from difflib import SequenceMatcher
    
    def normalize_text(text):
        """æ ‡å‡†åŒ–æ–‡æœ¬ï¼ˆä¸ question_service_v2 ä¿æŒä¸€è‡´ï¼‰"""
        if not text:
            return ""
        normalized = text.strip().replace('\n', '').replace('\r', '').replace(' ', '')
        normalized = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9]', '', normalized)
        return normalized.lower()
    
    # å¤„ç†æ‰€æœ‰æˆåŠŸçš„ç»“æœ
    success_results = [(i, r) for i, r in enumerate(results) if r.get('success')]
    
    if len(success_results) > 1:
        for i, (idx1, result1) in enumerate(success_results):
            if result1.get('is_batch_duplicate'):
                continue  # å·²ç»æ ‡è®°ä¸ºé‡å¤ï¼Œè·³è¿‡
            
            question_text1 = result1.get('question_text', '').strip()
            if not question_text1 or len(question_text1) < 10:
                continue
            
            normalized1 = normalize_text(question_text1)
            
            # ä¸ä¹‹å‰çš„æ‰€æœ‰é¢˜ç›®æ¯”è¾ƒ
            for j, (idx2, result2) in enumerate(success_results[:i]):
                if result2.get('is_batch_duplicate'):
                    continue
                
                question_text2 = result2.get('question_text', '').strip()
                if not question_text2 or len(question_text2) < 10:
                    continue
                
                normalized2 = normalize_text(question_text2)
                
                # ä½¿ç”¨ SequenceMatcher è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆä¸æ•°æ®åº“å»é‡æ–¹æ³•ä¸€è‡´ï¼‰
                similarity = SequenceMatcher(None, normalized1, normalized2).ratio()
                
                # ç›¸ä¼¼åº¦é˜ˆå€¼ 0.85ï¼ˆä¸æ•°æ®åº“å»é‡ä¿æŒä¸€è‡´ï¼‰
                if similarity >= 0.85:
                    result1['is_batch_duplicate'] = True
                    result1['duplicate_of_index'] = idx2  # åŸå§‹ç´¢å¼•ï¼ˆä»0å¼€å§‹ï¼‰
                    result1['duplicate_similarity'] = similarity
                    batch_duplicate_count += 1
                    logger.warning(
                        f"[BatchService] âš ï¸ æ£€æµ‹åˆ°æ‰¹æ¬¡å†…é‡å¤: "
                        f"é¢˜ç›®#{idx1+1} ä¸é¢˜ç›®#{idx2+1} é‡å¤ "
                        f"(ç›¸ä¼¼åº¦={similarity:.3f}, é˜ˆå€¼=0.85)"
                    )
                    break  # æ‰¾åˆ°ä¸€ä¸ªé‡å¤å³å¯ï¼Œè·³å‡ºå†…å±‚å¾ªç¯
            else:
                # æ²¡æœ‰æ‰¾åˆ°é‡å¤ï¼Œæ ‡è®°ä¸ºéé‡å¤
                result1['is_batch_duplicate'] = False
    
    # ç¡®ä¿æ‰€æœ‰ç»“æœéƒ½æœ‰ is_batch_duplicate å­—æ®µ
    for r in results:
        if 'is_batch_duplicate' not in r:
            r['is_batch_duplicate'] = False
    
    if batch_duplicate_count > 0:
        logger.warning(f"[BatchService] âš ï¸ æ‰¹æ¬¡å†…æ£€æµ‹åˆ° {batch_duplicate_count} é“é‡å¤é¢˜ç›®")
    else:
        logger.info(f"[BatchService] âœ… æ‰¹æ¬¡å†…æœªå‘ç°é‡å¤é¢˜ç›®")
    
    # ç§»é™¤indexå­—æ®µ
    for r in results:
        if 'index' in r:
            del r['index']
    
    # ç»Ÿè®¡
    total_time = time.time() - total_start
    success_count = len([r for r in results if r.get('success')])
    failed_count = len(results) - success_count
    
    # è®¡ç®—å¹³å‡æ—¶é—´
    success_results = [r for r in results if r.get('success')]
    avg_time = sum([r.get('total_time', 0) for r in success_results]) / success_count if success_count > 0 else 0
    
    # ç»Ÿè®¡å»é‡ä¿¡æ¯
    batch_duplicate_info = []
    database_cache_count = 0
    for r in results:
        if r.get('success'):
            if r.get('is_duplicate'):
                database_cache_count += 1
            if r.get('is_batch_duplicate'):
                dup_idx = r.get('duplicate_of_index', -1)
                dup_sim = r.get('duplicate_similarity', 0.0)
                batch_duplicate_info.append(f"é¢˜ç›®#{dup_idx+1}é‡å¤")
    
    logger.info(f"[BatchService] âœ… æ‰¹é‡å¤„ç†å®Œæˆ:")
    logger.info(f"   æ€»è€—æ—¶: {total_time:.1f}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)")
    logger.info(f"   æˆåŠŸ: {success_count}/{len(results)}")
    logger.info(f"   å¤±è´¥: {failed_count}/{len(results)}")
    logger.info(f"   å¹³å‡æ¯é¢˜: {avg_time:.1f}ç§’")
    logger.info(f"   æ€»è´¹ç”¨: Â¥{total_cost:.6f}")
    if database_cache_count > 0:
        logger.info(f"   ğŸ’¾ æ•°æ®åº“ç¼“å­˜å‘½ä¸­: {database_cache_count} é“é¢˜ï¼ˆèŠ‚çœè´¹ç”¨å’Œæ—¶é—´ï¼‰")
    if batch_duplicate_count > 0:
        logger.info(f"   ğŸ” æ‰¹æ¬¡å†…é‡å¤: {batch_duplicate_count} é“é¢˜")
    
    # è¿”å›æ•°æ®ç»“æ„ï¼šåŒ…å« results å’Œ statistics
    return {
        'results': results,
        'total': len(results),
        'success_count': success_count,
        'failed_count': failed_count,
        'total_time': total_time,
        'avg_time_per_question': avg_time,
        'total_cost': total_cost,
        # åŒæ—¶ä¿ç•™ statistics å­—æ®µï¼ˆå…¼å®¹æ–‡æ¡£æ ¼å¼ï¼‰
        'statistics': {
            'total': len(results),
            'success_count': success_count,
            'failed_count': failed_count,
            'total_time': total_time,
            'avg_time_per_question': avg_time,
            'total_cost': total_cost
        }
    }
