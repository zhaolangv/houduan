# 数据库连接池优化说明

## 📋 问题描述

在高并发批量处理时，出现以下错误：

```
QueuePool limit of size 5 overflow 10 reached, connection timed out, timeout 30.00
```

**问题原因**：
- 默认连接池大小只有 5 个连接
- 最大溢出连接数只有 10 个
- 批量处理时，多个并发任务同时访问数据库进行重复检测，导致连接池耗尽

---

## ✅ 优化方案

### 1. 增加连接池大小

已优化连接池配置：

- **PostgreSQL/MySQL**:
  - `pool_size`: 5 → **20**（连接池大小）
  - `max_overflow`: 10 → **30**（最大溢出连接数）
  - `pool_timeout`: 默认 → **30秒**（连接超时）

- **SQLite**:
  - `pool_size`: 默认 → **20**
  - `max_overflow`: 默认 → **30**
  - `check_same_thread`: `False`（允许多线程访问）
  - `timeout`: 30秒

### 2. 配置详情

#### PostgreSQL/MySQL 配置

```python
app.config['SQLALCHEMY_ENGINE_OPTIONS'] = {
    'pool_pre_ping': True,    # 检查连接是否有效
    'pool_recycle': 300,      # 回收连接时间（秒）
    'pool_size': 20,          # 连接池大小（默认5不够）
    'max_overflow': 30,       # 最大溢出连接数（默认10不够）
    'pool_timeout': 30,       # 获取连接的超时时间（秒）
}
```

#### SQLite 配置

```python
app.config['SQLALCHEMY_ENGINE_OPTIONS'] = {
    'pool_pre_ping': True,
    'pool_recycle': 300,
    'pool_size': 20,
    'max_overflow': 30,
    'pool_timeout': 30,
    'connect_args': {
        'check_same_thread': False,  # 允许多线程访问
        'timeout': 30                # SQLite 连接超时
    }
}
```

---

## 📊 性能提升

### 优化前

- 连接池大小：5
- 最大连接数：15（5 + 10）
- **问题**：批量处理 10+ 个并发任务时，连接池耗尽

### 优化后

- 连接池大小：20
- 最大连接数：50（20 + 30）
- **优势**：可以同时支持 50 个并发数据库连接

---

## 🔧 使用说明

### 自动应用

连接池配置会在应用启动时自动应用，无需额外配置。

### 验证配置

启动应用后，可以通过日志查看数据库连接状态：

```bash
# 查看启动日志
python app.py
```

或通过健康检查接口：

```bash
curl http://localhost:5000/api/health
```

---

## ⚠️ 注意事项

### SQLite 限制

SQLite 对并发支持有限：

- ✅ **读并发**：支持多线程读取
- ⚠️ **写并发**：同时写入会锁库，建议使用 PostgreSQL/MySQL

### 连接数建议

根据实际负载调整：

- **小规模**（< 10并发）：`pool_size=10, max_overflow=15`
- **中等规模**（10-30并发）：`pool_size=20, max_overflow=30`（当前配置）
- **大规模**（> 30并发）：`pool_size=30, max_overflow=50`

---

## 🔍 故障排查

### 问题1: 仍然出现连接池耗尽

**可能原因**：
- 连接没有正确释放
- 并发数超过连接池容量

**解决方案**：
1. 检查代码中是否正确关闭数据库连接
2. 减少并发数（`max_workers` 参数）
3. 进一步增加连接池大小

### 问题2: SQLite 并发写入错误

**症状**：
```
database is locked
```

**解决方案**：
1. 减少并发写入操作
2. 使用 PostgreSQL 或 MySQL（更好的并发支持）
3. 优化代码，减少数据库写入频率

---

## 📝 相关配置

连接池配置在 `app.py` 中自动设置，支持：

- ✅ PostgreSQL
- ✅ MySQL
- ✅ SQLite

---

## 🎯 最佳实践

### 1. 开发环境

可以使用默认配置或较小的连接池：

```python
pool_size=10
max_overflow=15
```

### 2. 生产环境

建议使用 PostgreSQL 或 MySQL，并配置合适的连接池：

```python
pool_size=20
max_overflow=30
```

### 3. 高并发场景

如果仍然遇到连接池问题：

1. 进一步增加连接池大小
2. 优化重复检测逻辑（减少数据库查询）
3. 使用数据库索引优化查询性能
4. 考虑使用 Redis 缓存重复检测结果

---

## 📄 相关文档

- `启动时预加载说明.md` - OCR 预加载说明
- `并发处理优化方案.md` - 并发处理优化方案
